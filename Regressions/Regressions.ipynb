{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Linear Regressions and Modeling Outliers\n",
    "\n",
    "Machine Learning by way of **regressions** is a way of making predictions on continuous output of some features. Linear Regressions are considered to be *continuous* supervised learning whereas classifiction algorithms are for *discrete* supervised learning.  The word *continuous* refers to the output being a continuous measurement. The equation for linear regression model is as follows:\n",
    "\n",
    ">$y(w,x) = w_{0} + w_{1}x_{1} + w_{2}x_{2} + ... + w_{n}x_{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Regressions in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [ 0.5  0.5]\n",
      "Intercept: 2.22044604925e-16\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "clf = LinearRegression()\n",
    "\n",
    "clf.fit([[0,0],[1,1],[2,2]],[0,1,2])\n",
    "\n",
    "print \"Coefficients:\",clf.coef_\n",
    "\n",
    "print \"Intercept:\", clf.intercept_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##How to measure Errors in Regressions\n",
    "\n",
    "Error in Regression modeling is defined as: \"The difference between the actual result and the predicted result\".  The purpose of a linear regression model is minimize the error between the data points so that it has the best fit.  Below are two metrics that we use to determine the best model fit for a set of data points\n",
    "\n",
    "###Sum of Squared Errors\n",
    "The equation to respresent the sum of squared errors (SSE) is:\n",
    "\n",
    ">$\\min\\sum_{All Training Points}(actual - predicted)^2$\n",
    "\n",
    "Algorithms used to perform this task are **Ordinary Least Square** and  **Gradient Descent**.  The reason that we take the sum of squared errors and not just the absolute error is because, there is ambuiguity in the sum of the absolute errors, and there can be multiple lines that minimize this value.  On the contrary, there can only be one line that minimizes the sum of the squared errors.  One of the shortcomings for SSE is that the value may increase due to having more data points, but that does not correlate to having a better/worst fit.\n",
    "\n",
    "###R-squared\n",
    "**R-squared** is metric score used to determine how well your model is doing.  When calculating the r-squared value on the training data, you can determine whether your model is overfitting or not.  R-Squared answers the questoion \"How much of my change in the output is explained by the change in my input. R-squared resides between 0 and 1 where 1 means that your model is doing a good job describing the relation between the input and output variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "Outliers in a regression model can cause for the line to not fit due to it trying to minimize the sum of the squared errors.  Outliers can be caused ba variety of factors from input malfunction, to human error.\n",
    "### Outlier Detection/Removal Algorithm\n",
    "The outlier detection and removal algorithm can be used to remove outliers from data set.  The steps are as follows:\n",
    "- Train your data\n",
    "- Remove the points that have the highest residual error\n",
    "- Train data again"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
