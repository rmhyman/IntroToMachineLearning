{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Validation\n",
    "Validation is done on machine learning algorithms to verify that your model is not overfitting and to give you an estimate on the performance of your model.\n",
    "\n",
    "##Train and Test Split\n",
    "When given a dataset, it is good to split the data into two categories:\n",
    "- Training data\n",
    "- Testing data\n",
    "\n",
    "The training data is used to fit your machine learning model whereas the testing data is used to check how well your model is performing.  It is good to have at least 10% of your data for testing (90/10 split). Below is an example of using train_test_split in sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "features_train,features_test,labels_train,labels_test = cross_validation.train_test_split(iris.data,iris.target,test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##K-fold Cross Validation\n",
    "\n",
    "k-fold cross validation is the concept of partitioning your dataset into k partitions and using each partition as the testing data and the other k-1 partitions as the training data.  The algorithm performs fitting and learning k times, and takes the average result of the k experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kf = cross_validation.KFold(len(iris.data), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##GridSearch Cross Validation\n",
    "\n",
    "GridSearch CV is a systematic way of performing parameter tuning on Machine Learning Algorithm.  It allows you to supply multiple values for a parameter and it will execute the machine learning algorithm using these different parameters and return the one with the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
       "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "parameters = {'kernel': ('linear','rbf'), 'C':[1,10]}\n",
    "svr = SVC()\n",
    "clf = GridSearchCV(svr,parameters)\n",
    "clf.fit(iris.data,iris.target)\n",
    "\n",
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Evaluation Metrics\n",
    "\n",
    "##Accuracy Score\n",
    "accuracy metric used in sklearn is defined as the # of items in a class labeled correctly divided by the # of all items in that class.  One of the shortcomings of the accuracy score is that it could give you deceiving good results if the class is skewed where there is a small number of items in the class.  This pretty much means that if you predict a high percentage in a small number of items, that may not scale well when given a large number of items.\n",
    "\n",
    "##Confusion Matrix\n",
    "Confusion Matrix is used to notate how many values you predicted correctly and how many actual values were there.  Confusion matrices illustrate how many false positives, true negatives and true positives you have predicted.  In a confusion matrix, we hope for the diagnol has the highest values, because those values represent the number of correct predictions.\n",
    "##Precision and Recall\n",
    "\n",
    "**Recall** is defined as the probability that your algorithm correctly predicts a value.  **Precision** is defined as the probability that your algorithm predicted a value and was correct.\n",
    "\n",
    "###True Positive\n",
    "*True Positives* are values in my confusion matrix where I predicted the value and it equals the actual value\n",
    "###False Positive\n",
    "*False Positives* are places where I predicted something to be true, but the actual value is false.\n",
    "###False Negatives\n",
    "*False Negatives* are places where I predicted the value to false, but the actual value was true.\n",
    "\n",
    "Precision can now be defined as:\n",
    ">$\\frac{truePositives}{truePositives + falsePositives}$\n",
    "\n",
    "and Recall can now be defined as:\n",
    ">$\\frac{truePositives}{truePositives + falseNegatives}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall score:  1.0\n",
      "precision score:  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ransf\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1172: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\ransf\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1082: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 7]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix,precision_score,recall_score\n",
    "\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "print \"recall score: \", recall_score(labels_test,pred,iris.target)\n",
    "\n",
    "print \"precision score: \", precision_score(labels_test,pred,iris.target)\n",
    "\n",
    "confusion_matrix(labels_test,pred,iris.target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
