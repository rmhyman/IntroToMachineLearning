{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Naive Bayes\n",
    "**Naive Bayes** is a classification algorithm used for a *supervised* machine learning problem.  An example of such problem would be determining who wrote some text based on prior information.  Before going into details about the algorithm, we need to clarify some terminology.  First let's discuss what are **features** and **labels**.\n",
    "\n",
    "##Features\n",
    "**Features** can be defined as details or information that you consider important to be able to classify or draw some conclusion about a particular item.  For example, If I wanted to classify cars, I may consider wheels, number of doors, interior, and engine size, as features for determining what type of car it is.  Features are important for *supervised learning* algorithms.\n",
    "\n",
    "##Labels\n",
    "**Labels** are used to determine how we classify different objects.  In a supervised Learning problem, We label our training and test data so that our Machine Learning algorithm can fit a model based on the given inputs.  By training our model, we can make better predictions about unseen data in the future.\n",
    "\n",
    "#The Naive Bayes Algorithm\n",
    "Naive Bayes algorithm can be used to determine the **decision surface** within a scatterplot of data points.  The Decision surface is defined as a surface in which we are able to classify how future data points should be labeled based on where they lie in the graph.  The Naive Bayes Algorithm in Python is apart of the *sci-kit learn* module.  Let's look at a small example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "features_train = np.array([[-1,-1],[-2,-1],[-3,-2],[1,1],[2,1],[3,2]])\n",
    "labels_train = np.array([1,1,1,2,2,2])\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(features_train,labels_train)\n",
    "\n",
    "print(clf.predict([-0.8,-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the accuracy of our model by running the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "labels_test = np.array([1])\n",
    "pred = clf.predict([-0.8,-1])\n",
    "accuracy = accuracy_score(pred,labels_test,normalize=True)\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course in this example, our accuracy is 100%, but that is not always the case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Bayes Rule\n",
    "**Bayes Rule** defines probablistic inference and is heavily used in statistics and artificial intelligence.  Bayes Rule starts with a *prior probability*.  The **sensitivity** of the test is the probability that the prior probability is true and the outcome is true.  The **specitivity** of the test is the probability that the prior probability is true and the outcome is actually false. Bayes Rule is defined as:\n",
    "\n",
    "> Prior probability + test evidence -> posterior probability\n",
    "\n",
    "The Algorithm to compute the posterior probability is as follows:\n",
    "- Calculate the joint probability. That is calculate the probability of the prior * evidence being true/false and calculate the probability of the negation of the prior * evidence being true/false.\n",
    "- Calculate the normalizer.  Given the joint probability will not sum of to one, it needs to normalized which is summing the results of the two joint probabilities above.\n",
    "- Divide the two joint probabilities by the normalizer to get the posterior probabilites.\n",
    "\n",
    "To make it more concrete, Suppose you were given:\n",
    "\n",
    "1. P(C) - prior\n",
    "2. P(Pos | C) - sensitivity\n",
    "3. P(Neg | C) - specitivity\n",
    "\n",
    "Then the Bayes Algorithm would go as follows:\n",
    "\n",
    "- Multiply P(C) and P(Pos | C) which will give joint probability P(Pos,C)\n",
    "- Multiply P(C) and P(Pos | ~C) which will give joint probability P(Pos,~C)\n",
    "- Sum the two products above to give you the P(Pos) which is independent of C\n",
    "- Divide The first product by P(Pos) which will give you the posterior probability P(C | Pos)\n",
    "- Divide The second product by P(Pos) which will give you the posterior probability P(~C | Pos)\n",
    "\n",
    "Naive Bayes is commonly used for text learning, that is, given the probability(or frequency) that two people use certain words, determine who wrote some unseen text.  Naive Bayes is \"naive\" because it allows you to determine how to label data given that the labels are hidden based only on the probability information given to you.  For example, using text learning, we can determine who wrote some text based on how frequent they used words, and we naively ignore the word order of their usage.  That is what makes Naive Bayes \"naive\".  Naive Bayes works very well on large feature spaces in text learning, but it can give inaccurate words when considering phrases that may have some meaning when paired together, but a different meaning if seperated.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
