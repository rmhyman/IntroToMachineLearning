{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Feature Scaling\n",
    "**Feature Scaling** is the concept that rescales the measurement of features so that each will have more of an equal factor when performing Machine Learning.  This scaling is usually reduced to a range between 0 and 1. The formula for feature scaling is the following:\n",
    "\n",
    ">$x_{rescaled} = \\frac{x - x_{min}}{x_{max} - x_{min}}$\n",
    "\n",
    "Some Algorithms that would be affected by Feature Scaling are:\n",
    "- Support Vector Machine with RBF kernel\n",
    "- K-means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Min/Max Scaler in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5       ,  0.        ,  1.        ],\n",
       "       [ 1.        ,  0.5       ,  0.33333333],\n",
       "       [ 0.        ,  1.        ,  0.        ]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "x_train = np.array([[1.,-1.,2.],[2.,0.,0.],[0.,1.,-1.]])\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "x_train_mms = mms.fit_transform(x_train)\n",
    "\n",
    "x_train_mms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Feature Selection\n",
    "\n",
    "Feature Selection is the concept of choosing the minimum number of features that have maximum amount of predictibility.\n",
    "\n",
    "The process of adding new features is:\n",
    "- Use your human intuition to determine whether that feature is a good selection or has some predictive power\n",
    "- Code up the new feature\n",
    "- Visualize the new feature through plots\n",
    "- Repeat\n",
    "\n",
    "It is sometime a viable option to remove features because they are too noisy, or may cause overfitting, or may be strongly related to an already present feature.  This feature removal can have the advantage of speeding up your algorithm process and generalize your model for better prediction.\n",
    "\n",
    "\n",
    "It is good to note that features and information are separate entities.  Features attempts to access information, but are not information themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Feature Selection in sklearn\n",
    "Feature selection can be done by an individual component (*SelectPercentile*, and *SelectKBest*) or it can also be done inside of the Text vectorizer component, i.e *TfidfVectorizer*.  The parameters to control feature in the Tfdif vectorizer are:\n",
    "- max_df\n",
    "- max_features\n",
    "- stop_words\n",
    "\n",
    "All of those parameters removes words that have surpassed some sort of threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif, SelectPercentile\n",
    "\n",
    "sel = SelectPercentile(f_classif, percentile=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above shows how to perform feature selection based on a percentile threshold.  This function would go through all of the features and choose the top 10% of features which (hopefully)generate the most information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Bias Variance Tradeoff and Feature Selection\n",
    "\n",
    "If you use a few features, you run the risk of having a **high-bias** model due to it being oversimplified.  Conversly, if you a lot of features, you run the risk of having a **high-variance** model due to it being overfitted.  This is why choosing the right number of features becomes important in creating a generalized model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Regularization (Lasso Regression)\n",
    "\n",
    "**Regualarization** is the method for penalizing extra features within your regression model.  **Lasso Regression** performs regularization by using the following formula:\n",
    "\n",
    ">$\\min SSE + \\lambda|\\beta|$\n",
    "\n",
    "where $\\lambda$ is a penalty parameter and $\\beta$ is the coefficient vector of my regression model\n",
    "\n",
    "features that do not help the regression model have a coefficient that is set to zero.  Lasso Regression performs this by adding each feature one at a time and sets the coefficient based on the formula above.\n",
    "\n",
    "\n",
    "##Lasso regression in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "regr = Lasso()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the coefficients of *regr* by calling the attribute **coef_**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
