{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Decision Trees\n",
    "**Decision Trees** are one of the oldest classification algorithms it's methodology is fairly straightforward.  It has the ability to construct non-linear decision boundaries by answering multiple linear questions.  Based on the response to these questions, the tree splits into two seperate outcomes in which you have reached a leaf node (you are able to classify the outcome based on this decision) or you continue with another question to make a decision on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Decision Trees in sklearn\n",
    "\n",
    "To use Decision Trees in sklearn, we use the **DecisionTreeClassifier**.  The class has various parameters such as *criterion*, *min_samples_split*, and *max_depth* which can be useful for tuning your Decision Tree and to avoid overfitting. Below is an example of using Decision Trees in sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "features_train = [[0,0],[1,1]]\n",
    "labels_train = [0,1]\n",
    "features_test = [[2.,2.]]\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "clf.fit(features_train,labels_train)\n",
    "\n",
    "pred = clf.predict(features_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid overfitting the training data in Decision Trees, we can use the *min_samples_split* parameter in the DecisionTreeClassifer.  This parameter defines the smallest number of samples that we can have in a leaf node.  Any number greater than this would prompt for the Decision tree to split again if possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Entropy\n",
    "Another important concept to Decision Trees is **entropy**.  Entropy is defined as the amount of impurities in a number of examples.  It is used to control how a Decision Tree decides where to split the data. Entropy is mathematically as:\n",
    "\n",
    ">$\\sum_{i}-P_{i}log_{2}(P_{i})$\n",
    "\n",
    "where $P_{i}$ equals the fraction of examples in class *i*.  For example, suppose that we had 4 students where 2 were freshmen and 2 were seniors and one of each went to Univ. of South Florida and Bethune-Cookman University.  If we split the data by universities, Then the $P_{USF}$ would be 0.5 and the $P_{BCU}$ would be 0.5 for both freshmen and seniors.  It is worth noting that if all the examples belong to the same class, then the entropy value is **zero**.  If the examples are evenly split across the classes, then the entropy is the maximum value of **one**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Information Gain\n",
    "Information gain uses entropy to decide which features it should choose to split the data.  The objective of the Decision Tree Algorithm is to maximize the information gain at each step.  Information gain is defined as:\n",
    "\n",
    ">Information gain = entropy(parent) - WeightedAverage(entropy(children))\n",
    "\n",
    "Information gain can be used in sklearn by setting the *criterion* parameter to \"entropy\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Bias Variance Dilemna\n",
    "\n",
    "A high biased algorithm is one that pretty much ignores the data and is very biased towards some decision.  A high variance algorithm is one that is pretty preceptive of the data and it can only replicate situations that it has seen before.  In machine learning algorithms, you want to control the amount of bias-variance so that your algorithm is better at making generalizations.  One of the tasks of creating powerful machine learning algorithms is tuning the parameters such that you control the bias-variance tradeoff to where it is somewhere in the middle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Pros and Cons\n",
    "The strengths of Decision Trees is that they very easy to use and are easier to visualize how the decisions are being made compared to other classification algorithms.  The weakness of Decision Trees are that they are prone to overfitting.  Thus you have to be careful of how you fine tune your parameters to avoid this pitfall."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
