{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Support Vector Machines\n",
    "**Support Vector Machines** or **SVM** defines the decision boundary between two classes.  SVM defines this decision boundary by maximizing the distances between the points in the two classes. This distance is known as the **margin**.  Although SVMs are designed to give the largest margin between the two classes, it can tolerate *outliers* which may not allow the decision boundary to uniformly split the two classes.\n",
    "\n",
    "##SVM in sklearn\n",
    "\n",
    "Now let's see how SVMs can be coded in Python  Below is the example given in the documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "features_train = [[0,0],[1,1]]\n",
    "labels_train = [0,1]\n",
    "features_test = [[2.,2.]]\n",
    "clf = svm.SVC()\n",
    "clf.fit(features_train,labels_train)\n",
    "pred  = clf.predict(features_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Nonlinear SVMs\n",
    "\n",
    "Traditionally SVMs generate linear decision boundaries, but it is possible to give intricate nonlinear decision boundaries as well.  This is done by adding additional features in which the plots can be transformed into a different plane such that a linearly decision boundary can be made.  Once the boundary has been defined in the transformed plane, then when we map the results back to original plane, it gives us a nonlinear decision boundary.  These transformations can be performed by the *kernel* in python.  In esscense, this is what happens:\n",
    "\n",
    ">$x,y -> x_{1},x_{2},x_{3},x_{4}$ (Not Seperable -> Seperable)\n",
    "\n",
    ">Non Linear Seperation <- Linear Seperable Solution\n",
    "\n",
    "The scikit-learn module provides various kernels for you use by passing in the parameter to the *kernel* parameter of the SVC().\n",
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel=\"linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other two parameters that can have an affect on how the decision boundary is generated are the **gamma** and **C** parameters.  The *gamma* parameter defines the influence of a single training example(low value means far reach, and high value means close reach).  The **C** parameter determines whether to tradeoff misclassification of training examples versus the smoothness of the decision surface (low value means smooth decision surface and high value means we try to classify all training examples correctly).  It is worth noting that based on your kernel, gamma, and C parameters, you run the risk of **overfitting** which is something you want to avoid in machine learning algorithms as they make it difficult to predict unseen data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Pros and Cons\n",
    "\n",
    "SVMs work really when there is a clear margin of seperation between the two domains.  They don't work as well when there is a very large dataset as algorithm can run at $O(n^3)$ or if there is a lot of noise (overlapping)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
